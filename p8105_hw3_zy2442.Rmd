---
title: "Homework 3"
author: Zhongqi Yue
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
```{r}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and ... columns.

Observations are the level of items in orders by user. There are user / order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles, and which are most items from?

```{r}
instacart %>% 
	count(aisle) %>% 
	arrange(desc(n))
```


Let's make a plot

```{r}
instacart %>% 
	count(aisle) %>% 
	filter(n > 10000) %>% 
	mutate(
		aisle = factor(aisle),
		aisle = fct_reorder(aisle, n)
	) %>% 
	ggplot(aes(x = aisle, y = n)) + 
	geom_point() + 
	theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table!!

```{r}
instacart %>% 
	filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
	group_by(aisle) %>% 
	count(product_name) %>% 
	mutate(rank = min_rank(desc(n))) %>% 
	filter(rank < 4) %>% 
	arrange(aisle, rank) %>% 
	knitr::kable()
```

Apples vs ice cream..

```{r}
instacart %>% 
	filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
	group_by(product_name, order_dow) %>% 
	summarize(mean_hour = mean(order_hour_of_day)) %>% 
	pivot_wider(
		names_from = order_dow,
		values_from = mean_hour
	)
```

## Problem 2

Load, tidy, and otherwise wrangle the data.

```{r}
accel_df= (
  read.csv("./data/accel_data.csv")%>% 
  janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute_of_a_day",
      names_prefix = "activity",
      values_to = "activity_counts") %>%
  mutate(minute_of_a_day = (as.numeric(as.factor(minute_of_a_day)))) %>% 
    mutate(
      weekday_vs_weekend = case_when(
        day != c("Saturday","Sunday") ~ "weekday",
        day == c("Saturday", "Sunday") ~ "weekend",
        TRUE                           ~ ""))
      
    )
```

After tidy the dataset, there are `r nrow(accel_df)` observations and `r ncol(accel_df)` variables, which are `r names(accel_df)`.

```{r}
accel_df= (
  read.csv("./data/accel_data.csv")%>% 
  janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute_of_a_day",
      names_prefix = "activity",
      values_to = "activity_counts") %>% 
  mutate(minute_of_a_day = (as.numeric(as.factor(minute_of_a_day))))%>% 
    mutate(
      weekday_vs_weekend = case_when(
        day != c("Saturday","Sunday") ~ "weekday",
        day == c("Saturday", "Sunday") ~ "weekend",
        TRUE                           ~ "")))
  
  accel_df$day = factor (accel_df$day, c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday"))
  
accel_df %>% 
  group_by(week, day) %>% 
    summarize(total_activity = sum(activity_counts, na.rm = TRUE)) %>%
  arrange(week,day) %>% 
    mutate(activity_rank = min_rank(total_activity))%>% 
    	knitr::kable()
```

There is no obvious trend over total activity for each day during these three weeks. However, to some degree, the total activity counts for Tuesday and Wednesday are relatively stable, which are always within 350000~470000 counts across these three weeks. Besides, the rank of the total activity also shows that the rank of Tuesday is relative stable compared to other days, which always ranks 2,3 or 4 within a week. 

```{r}
accel_plot = 
  accel_df= (
  read.csv("./data/accel_data.csv")%>% 
  janitor::clean_names() %>% 
    pivot_longer(
      activity_1:activity_1440,
      names_to = "minute_of_a_day",
      names_prefix = "activity",
      values_to = "activity_counts")
  ) %>% 
  mutate(minute_of_a_day = (as.numeric(as.factor(minute_of_a_day))))

accel_plot %>% 
  	ggplot(aes(x = minute_of_a_day, y = activity_counts, color = day)) + 
	geom_point(alpha = .5)+
  geom_line()
```

The scatter plot indicates the pattern that the male has relatively higher activity counts during the 250~280 minutes for mostly Monday, Friday and Saturday; 550~1125 minutes for mostly Thursday, Friday and Sunday over weeks. For Tuesday and Wednesday, the activity counts are relatively stable over the whole day compared to other days. 

## Problem 3

```{r}
library(p8105.datasets)
library(ggpubr)
data("ny_noaa")
```

This dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. The observations are some weather data -- Precipitation, Snowfall,Snow depth, Maximum temperature, Minimum temperature of different weather stations. There are 7 variables, which contains character variables -- id,tmax,tmin; date variable -- date;integer varibales -- prcp, snow, snwd. The missing data in each variable could be an issue since we might not know the weather changes as time went by. 

```{r}
ny_noaa_df=
  ny_noaa %>% 
  separate(date, c("year","month", "day")) %>% 
  mutate(
    year = as.integer(year),
    month = as.integer(month),
    day = as.integer(day)) %>% 
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         year = as.character(year),
         month = as.character(month),
         tmax = tmax / 10,
         tmin = tmin / 10)

ny_noaa_df %>% 
  count(snow) %>% 
  arrange(desc(n))
```

For Snowfall, 0 is the most commonly observed value since it shows up most often among all observed values. Similarly, 25 is the second most commonly observed values; 13 is the third most commonly observed values. 

```{r}
ny_noaa_df %>% 
  filter(month %in% c("1","7")) %>%
    mutate(month = recode(month, "1" = "January", "7" = "July")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax))%>% 
  ggplot(aes(x = year, y = mean_tmax, color = id)) + 
  geom_point(alpha = 0.5)+
  geom_smooth(se = TRUE,method = "lm", color="red")+
  theme(legend.position = "none")+
	theme(axis.text.x = element_text(angle = 45, vjust = 0.5, hjust = 0.5))+
  facet_grid(. ~ month)+
  labs(
    title = "The average max temperature in January and in July in each station across years"
    )
```

As the plot shown, it is obvious to see that July is much warmer than January in every year and every station. For January, the temperature tends to be warmer from 1981 to 1990, and then tends to decrease until 1994. Then the temperature tends to increase until 1998 and keep decreasing until 2004. Lastly, it tends to be warmer again until 2006 then decrease until 2010. It seems that the average tmax in January was the warmest in 1991 and coldest in 2004. For July, the average tmax is more stalbe across years compared to January. It seems that 2010 is the hottest year for July and 2000 is the coldest. For January, it seems that there are no obvious outliers. For July, it seems that there is one outlier in 1988, which indicates that the average max temperature in July of this station was much lower than other stations in 1988.  

```{r}
plot1 = 
ny_noaa_df %>% 
  ggplot(aes(x = tmin, y = tmax))+
  geom_bin2d()+
   labs(
    title = "tmax vs tmin density"
    )

plot2 = 
  ny_noaa_df %>% 
  filter(snow > 0,
         snow <100) %>% 
  ggplot(aes(x = year, y = snow, fill = year))+
  geom_violin(alpha = 0.5)+
  stat_summary(fun = "median")+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))+
  scale_y_continuous(
    breaks = c(0, 50, 100),
    labels = c("0 mm", "50 mm","100 mm")
  )+
   labs(
    title = "The distribution of snowfall values (0~100mm) by year"
    )

ggarrange(plot1, plot2, 
          labels = c("A", "B"),
          ncol = 2, nrow = 1)
```

For this problem, in order to merge these two plots, the package "ggpubr" is installed and the function "ggarrange" is used. From plot A, it seems that the distribution of "tmax vs tmin" is more dense in the area where tmin ranges from 0 t0 40 degree C and tmax ranges from 0 to 25 degree C. From plot B, it is obvious that the distribution of snow fall values between 0 ~ 100 mm is very similar across years.  

